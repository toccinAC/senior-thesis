%
% $Id: conclusion.tex
%
%   *******************************************************************
%   * SEE THE MAIN FILE "AllegThesis.tex" FOR MORE INFORMATION.       *
%   *******************************************************************
%

\chapter{Discussion and Future Work}\label{ch:conclusion}

\section{Summary of Results}
% A discussion of the significance of the results
% and a review of claims and contributions.

% Significance of results
The results of experimentation support the claims of this research. The research suggested that there are pseudo-tested methods that can be found in systems that are loosely typed. Python is a loosely typed language, and it allows for a variable to be easily overwritten and for a decorator to perform this action.
% Pseudo-tested methods were found in almost every system.
The research has found that pseudo-tested methods were detected in almost every system that was tested. Nine out of ten systems that were tested through the usage of Function-Fiasco had functions that were being pseudo-tested. Overall, out of the 136 functions that were ``fiascoed,'' 72 were found to be pseudo-tested in some fashion. To be considered pseudo-tested the functions needed to be pseudo-tested in at least one test. While this qualification may sound weak, the definition of a pseudo-tested method requires that a test it is executed in will pass regardless of the output of the pseudo-tested method. So while it may be considered strongly tested if the other tests result in a failure, it is still pseudo-tested nonetheless. It is also important to mention the number of pseudo-tested methods found in each system. Function-Fiasco reported low numbers when it ran its execution. Low numbers of pseudo-tested methods in a system is a good thing. This means that the functions that the system is testing can be considered strongly tested. The functions still may be susceptible to other types of errors that result from edge cases, but when considering the expected input into them, the tests execute properly. The fuzzing in Function-Fiasco does perform this edge case exploration, but it is mainly used to determine that any case except for the known input will pass the test.

% Systems that boast high statement coverages tend to have a higher number of pseudo-tested methods
This research also suggested that systems that boast higher statement coverages are found to contain a higher number of pseudo-tested methods. The data produced in this experiment does support this suggestion. It was found that the number of pseudo-tested methods trended upwards as the statement coverage reported by each system increased. An explanation may be that the systems that have higher coverage just introduce more possibilities to pseudo-test functions. This supports the idea that systems that have high coverage are at a greater risk of pseudo-testing their functions. Striving to have a high coverage may lead to writing tests that are not testing something in a meaningful way or accidentally not testing anything at all. It could be argued that the systems with lower statement coverage are writing more meaningful tests that are testing areas that are at great risk in their execution for bugs. Ensuring that every line of code has a test associated with it can also produce a test that is just checking to make sure that something is returned, this can be found in the test suite for GatorGrader. Many of the tests were simply checking that something was returned and that the system did not crash. While these tests do have a purpose, they are considered to be pseudo-tested. This is because they are just checking that something exists, not that what was returned was correct. A test such as this cannot be considered to be fully tested as there is a whole aspect of its functionality that is not being accounted for. Function-Fiasco accounts for the fact that a function is being pseudo-tested by providing an adjusted function coverage that is representative of the actually tested functions of the system.

 % Systems that are using many tests to test one function are producing scenarios where the function is being pseudo-tested
As stated before, the research heavily indicates that the number of tests is the reason for the number of pseudo-tested methods. This is signficant because it means that having meaningful tests is the best practice to ensure that the system is fully testing a function. It is important to say though that statement coverage should also be acknowledged. The purpose of the research was not to explain that statement coverage is a metric that should be ignored. It was meant to leverage the use of statement coverage while still understanding what coverage is. Coverage simply is how much of the system is being executed during the testing phase~\cite{okken_2018}. Coverage is a useful metric, however it does not provide information as to whether or not the test suite has a high fault detection effectiveness. The results indicate this as well. In the systems with the highest statement coverage, almost half of the ``fiascoifiable'' functions in each system were found to be pseudo-tested in some manner. So while it is important to know that all of the system is being executed through the test suite at some point, it does not provide information on the effectiveness of the test suite.

% Why are the results significant
The results of the experiment were significant because they emphasize the danger that pseudo-testing presents. Not every function that was present in every system had the capability to be ``fiascoed.'' If pseudo-tested methods were found in this quantity, the number that may exist in complex systems could be wildly greater than initially expected. Once other types of returns are examined, the number may scale per type. The complexity of the system absolutely plays a role in the number of pseudo-tested methods present. A function may be called indirectly by a wide variety of others, which makes the parent structure that was mentioned before a massive spider web of function calls. This means that a function that is pseudo-tested is affecting much more of the system than thought of before. The number of pseudo-tested methods found in systems with high statement coverage with just primitive types calls for a greater emphasis on the quality of test written. It is very important to understand that the results of this research are an underapproximation. Due to the few cases that Function-Fiasco can handle, there is the possiblity that there are more cases that could lead to further discovery of pseudo-tested methods in a system. The number of pseudo-tested methods that Function-Fiasco will find after further development will not diminish as the further development will only increase the number of functions that can be investigated.

% Major Contributions
The major contributions of this research is as follows:

\begin{enumerate}
  \item A working automatic detection tool for pseudo-tested methods.
  \item A table that contains the results of a system's coverage and fault detection effectiveness.
  \item A revised metric that describes how accurately a system is able to detect errors.
  \item Two Pytest plugins that aid in the testing of software.
\end{enumerate}


\section{Discussion}
% Field Connections
  % * Software Engineering
This research is heavily connected to the field of Software engineering. Software engineering may be defined as the systematic design and development of software products and the management of the software process~\cite{IEEE}. This design is employed using aspects of many other processes that are crucial to the creation of software. The different aspects include design, architecture, implementation and testing. Function-Fiasco connects by examining the result of these processes to ensure that the system that was designed is behaving as the developers have envisioned. It also aims to ensure that the systems that the developers have put in place to check this behavior is written in a way that allows developers to accurately detect faults in their implementation. Overall, Function-Fiasco applies to the field of Software engineering because it is a system that developers can use to ensure that they have implemented their source code so that it acts in the way that was first perceived. It can be used at any point in the implementation if there are test cases that exist already, as this is the workflow that test-driven developement follows.

% * Software Testing
Function-Fiasco is also very heavily connected to the field of Software testing. How development teams test their software can influence the overall product after the development process has completed. It is the process of ensuring that the functionality of the system that has been created is operating in a manner that was intended and expected. This aspect of the development cycle can help to avoid major issues in the execution of the system, such as a crash. It can also ensure that the output of the system is what was expected. If this phase is not accomplished correctly or even at all, the software should be considered unstable to a point where the output in the scenarios that were expected may not be what is provided to the user. Assuming that the developers have written tests for their systems, Function-Fiasco can be used to ensure that the tests have been correctly implemented. Since test suites can be considered an examination of the system, and is often created by the team that developed the software that the test suite is analyzing, it is important to ensure that the tests have been written in a manner that is actually testing the behavior of the system. Function-Fiasco determines if the tests that exist in the test suite are of good quality. It can help determine what is not testing anything so that the tests can be addressed and the function can have proper tests written for it so that the output is being accurately tested.

% * Computer Science
Function-Fiasco is also connected to the greater field of computer science. Software is the way that the user interacts with the hardware in the computer system to perform a meaningnful operation. Function-Fiasco now provides a way to automatically detect faults in a test suite. This means that the developement process has another tool that allows developers to ensure that the software that they are creating is operating properly. This research also shows that the idea of a pseudo-tested method is not localized to the Java language, as this was proven in \cite{niedermayr2016will}. Psuedo-tested methods could potentially be an issue in a wide variety of software that is outside of the umbrella of Python and Java based systems. This greatly affects the field of computer science. This research also calls into question the current metrics that developers can use to ensure that their system is being tested. This extends to languages outside of Java and Python.

% Major Implications
This research could have major implications based on its results and the results of its predecessors. It has been proven that pseudo-tested methods exist in systems that have both high and low percentages of coverage.
  % * Change in the view of tested software
  This means that the definition of tested software stands to change. Currently high coverage can be observed as an indication of testedness. However as proven with this research, high coverage can also create scenarios in which functions of the system are not being adequately tested. Testedness should be reflected by the fault-detection effectiveness of the system. This research also suggests a change in how tested software is viewed. Bugs are common in software, this is because the development process of a system is never finished. After the development cycle has completed, the maintenance of the system begins. In this phase, bugs are made aware to the development team. Once the bugs have been signaled, the developement process cycles and begins again. This process continues because bug fixes involve more implementation that could result in further bugs. There is also the issue of technical debt. Technical debt is defined as code that may currently be working, but in the future, after additional development, may not function as intended~\cite{verdecchia2018architectural}. Pseudo-tested methods may be hiding operations that can be defined as technical debt. Since they are pseudo-tested, even after they are not behaving as they should, they will still result in passing tests. For these reasons, the research conducted with Function-Fiasco has revealed potential flaws in how developers view tested software.

  % * Possible change in the software testing process
  Function-Fiasco also presents the possiblity to change how the process of software testing is operated. Testing in most software engineering models happens late in the developement cycle. Function-Fiasco provides a tool to test not just the system source code during this phase. It allows for the testing of the test suite as well. Analyzing the test suite of systems should also be performed in the development cycle. This is because the tests are how the functionality of the system is determined to be correct. Integrating the process of evaluating the suite into the overall testing phase is imperative. It should not be assumed that functionality is correct until the tests are determined to be written correctly as well. This idea works properly even if the development team is operating under test-driven development. Once the tests are written and functionality begins development, it is easy to test the suite as well. After functionality that fits the test is written, the test could also be analyzed at the same time. This would allow further development knowing that the functionality is implemented properly to a higher standard of certainty. The research performed in the developement of Function-Fiasco and the experiment that followed allows the capability to change the way that the testing process of systems is conducted. It would allow a higher degree of certainty that the system is operating properly.

\section{Future Work}
The implementation on Function-Fiasco and its supporting plugins is not finished and there are many plans for further work on them as well as plans for future testing after these changes are implemented. There is much work that can be done in terms of features and bug fixes. Each addition or fix may lead to further pseudo-tested methods being found in each system.

% Bug Fixes and feature implementation ~ Function Fiasco
Function-Fiasco is the flagship system in the Function-Fiasco organization and it stands to have the most amount of work completed on it. Function-Fiasco has a number of features that need to be added to its functionality so that the system is able to analyze more functions and tests than it currently does.
% Update to the different types that are availble for fiascoifying
The main work that needs to be accomplished is an update to the different function returns that are not being currently analyzed. As stated before, the only functions that can be tested are ones that return a primitive type which include \texttt{integers}, \texttt{floats}, \texttt{booleans}, and \texttt{Strings}. Other types of functions that the system needs to be able to handle are ones that return container objects such as \texttt{lists}, \texttt{sets}, \texttt{tuples}, and \texttt{dictionaries}. The reason that these containers have been chosen is because they can be easily indexed in different ways. The most complex type out of this grouping is the \texttt{dictionary}. The reason being is that each index is a key-value pair, which may be difficult to fuzz depending on what each part of the key-value pair is. After the different container objects are implemented, the next feature that Function-Fiasco needs to be able to handle includes objects that have attributes. This was left out due to time constraints, but it will work in the same fashion as a list. Function-Fiasco will systematically proceed to fuzz each attribute that the object has. This is to ensure that whatever aspect of the function is being tested, the test will have an input of a fuzzed variable.
% Parameterized testing
The next crucial feature that will be implemented into Function-Fiasco will be the ability to discern paramaterized tests. Currently, each test in the paramaterized grouping must pass for the test to be considered pseudo-testing a function. In the future, each test will be looked at as its own entity and the input that the test uses will be used to separate the test grouping.

Function-Fiasco also needs to have an update in the number of tests that it discovers and the number of tested functions that it returns. Currently the number of tested functions of the systems is lower than expected. Systems that have a coverage of almost 100\% are being found to have a function coverage that is not near the 100\% coverage that is expected. This has a direct effect on the metrics that are being produced from Function-Fiasco. The function coverage after Function-Fiasco completes its execution is only going to be diminished due to the presence of pseudo-tested methods. Therefore the initial function coverage and the updated function coverage may be lower than expected. A reason that this may be occuring could be an issue in either pytestFinder or Function-Fiasco. Based on the configuration of some of the systems, running in another directory with a different environment may cause some of the tests to not execute properly. They may be looking for certain conditions that the current evironment does not have, causing the test to error and not release any information on what functions were tested in that execution. This would cause the function coverage to not be as high as it could be because functions were not counted because of the errors. Another issue that may be causing the lower function coverage would be the function discovery in the tests that are produced by pytestFinder. PytestFinder produces only the tests that need to be run because of what functions are being called in them. If pytestFinder does not correctly find all of the function calls in the test, the test will not be run. If this happened throughout the execution for that function, it would not be recognized as tested. Fixing this issue would cause all metrics associated with the number of functions that are tested to increase.

  % Ability to operate on other OS
  A bug that needs to be fixed is what operating systems Function-Fiasco is compatible with. As of now, the only operating system that Function-Fiasco will operate on is MacOS. The plan to fix this bug is to investigate the versions of the dependencies that each system is using. If this is an issue, Function-Fiasco will be shipped with a requirements document that has the updated versions that Function-Fiasco is currently using on MacOS. The other possiblity that may be forcing this issue could be that packages that are downloaded in the system are running at the same time forcing those that Function-Fiasco requires to operate to fail. The fix for this would be to observe what packages are running during the time of Function-Fiasco to determine where the issue is.

  % Documentation
  The documentation for Function-Fiasco needs to be updated as well. Much of the developement for Function-Fiasco was causing functionality to change at a rapid pace. Documenting the changes became an almost sisyphean task. Once the documentation could have been updated and correct, the functionality would change or a dependancy would be added. Therefore the documentation phase of development was difficult to maintain.

% Bug Fixes and feature Implementation ~ pytestFinder and pytestReload
Both pytestFinder and pytestReload need to have documentation updated as both have changed drastically since their conception. However the most significant portion of future work for both of the plugins is a feature that had to be removed from pytestFinder to ensure that the numbers were correct for the experimentation. The beginnings of a system to determine what functions were specifically called during a test were being implemented. This would allow pytestFinder to determine which functions were called based on the logic of the system. As of now, pytestFinder determines which functions and test to run based of AST nodes from the tests and source code. In the future, pytestFinder needs to determine what tests to include in the execution based on what will actually be executed based on the input of each test. This would also aid in the creation of the feature that allows Function-Fiasco to execute paramaterized tests on an individual basis, rather than the whole grouping. This whole grouping forced Function-Fiasco to count more pseudo-tested methods than actually existed.

  % Missed tests and not accurate function coverage
% Additional testing on other open source systems
The final piece of known future work is that of retesting. As more features are added to the functionality of Function-Fiasco, the same systems need to be examined again to determine if the current claims from this research remain consistent as the number of ``fiascoed'' functions increase. As stated previously, the planned future development involves adding cases that Function-Fiasco can handle. This development would expose other parts of the systems to further scrutiny that was not possible prior at the time of writing. This increase in scrutiny will result in the number of ``fiascoed'' methods increasing, and in theory increasing the number of pseudo-tested methods. Further examination of systems with Function-Fiasco could potentially have a greater affect on the coverage that the systems boast. This information would only support the idea that pseudo-tested methods are a prevalent issue that needs to be addressed. It will also provide more insight on which return types are the highest danger of pseudo-testing.

Research should also be performed on the different types of coverage. Even though statement coverage is a widely used metric to determine how much of a system is being tested, it is not the most stringent. There are other types of coverage that are more stringent and encompassing than statement coverage, such as fully defined mutation testing and modified condition coverage. When using other types of coverage that are more stringent, research may show that systems that still have a high coverage may have less pseudo-tested methods present due to more conditions being checked.


\section{Conclusion}
% Overall issue of pseudo-tested methods
The purpose of this research was to prove that pseudo-tested methods exist in systems that are based in Python. The experimentation proved this. Pseudo-tested methods were found in almost every system, regardless of the size or statement coverage. Out of the 10 systems that were examined, 9 were found to include functions that were being pseudo-tested. This finding is very important as it shows that this is an issue that is very prevalent and needs to be addressed. The idea of pseudo-tested methods was not conceived until 2016~\cite{niedermayr2016will}. They are a very new idea in computer science and are not being acknowledged in great detail. This research shows that the way that development teams are testing their software is creating cases that are not fully analyzing the behavior of the system.
 % * Pseudo-tested methods are a prevelant issue
 % * Found in almost every 9/10 systems that were tested

% Updated understanding that coverage could be a misleading metric
This research was also meant to provide insight into an aspect that coverage may not be providing information for. The fault detection effectiveness of a system may not always be indicated by the coverage of a system. A system having 100\% coverage does not mean that the tests that it is employing can be considered good tests. As shown previously, some tests were merely created so that the developer can determine if the system is simply returning something as an output. This can be seen by tests in GatorGrader that are just ensuring that the count of an element is greater than one. A case that this allows the test to pass is a number that can be considered high for the system. Since it is allowing information that is greater than the number one, if the system counts too many items, the number that is being compared may be very high. This will still pass the test case. While the test is accomplishing what the developer intended, it is still not ensuring that the number that it receives is correct. This research also found that the systems with the highest percentage of statement coverage had the highest number of pseudo-tested methods contained in their source code. These aspects suggest that more should be considered when determining if the system is being tested thoroughly. This research does not conclude that coverage is always misleading. It is still important to understand how much of the system is under scrutiny. The point of the research was to provide insight into the fault detection effectiveness of test suites.
 % * Systems with higher statement coverage had more pseudo-tested methods
 % * Some tests were used to ensure that the system did not crash

The findings of Function-Fiasco have also supported many of the conlcusions that were made in past studies. Chapter \ref{ch:relatedwork} presented the related work that has already been completed on this topic. Those studies have proven that pseudo-tested methods exist, specifically in Java based systems. They also provided a rationale that coverage may not always be a misleading metric. The research conducted during the execution of Function-Fiasco prove that pseudo-tested methods exist in Python based systems. A very interesting observation is made in \cite{vera2017comprehensive} is that other languages than Java may contain pseudo-tested methods. They concluded that their research could not be generalized to other languages. Their work was limited to the Java programming language, so they were not able to conclude if the same conditions could be found in other languages. Function-Fiasco has not only supported their research to prove that pseudo-tested methods exist, but it shows that this is an issue that is not only limited to Java. Function-Fiasco also supports the idea coverage as a metric is not an entirely misleading metric. Coverage is useful in understanding how much of the system is being executed during the evaluation of a system, but it does not indicate that the test suite has a high fault detection effectiveness.

% Ability to test for them automatically
Lastly, Function-Fiasco provides developers a way to evaluate their test suites. Prior to Function-Fiasco, the most commonly used metric was that of coverage. The issue with coverage is that it does not provide an indication as to the fault detection effectiveness of the system. Coverage cannot determine the quality of the tests, only how much is being tested in them. Function-Fiasco has the ability to determine the quality of the test, which means is it actually testing what was intended. Based on the size and complexity of systems, it can be very difficult to check each of the systems by hand. Pseudo-tested methods by nature are difficult to find because they are masked by a continuously passing test case, so checking for them by hand may not find all of them. It will also take a long time to check every case, and by the time this has been accomplished, the test suite may have changed forcing the evaluation to begin again. Function-Fiasco provides the ability to detect pseudo-tested methods automatically. It finds pseudo-tested methods using a combination of mutation testing, fuzz testing, and fault injection to determine if a function is being tested properly.
 % * Nearly impossible to test each of the cases of a pseudo-tested method by hand
 % * Function-Fiasco presents the possibility of testing for these functions automatically
