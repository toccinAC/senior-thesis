%
% $Id: ch02_relatedwork
%
%   *******************************************************************
%   * SEE THE MAIN FILE "AllegThesis.tex" FOR MORE INFORMATION.       *
%   *******************************************************************
\chapter{Related Work}\label{ch:relatedwork}

% A typical second chapter deals with a survey of the literature
% related to the thesis topic. The subsections may be organized in whatever
% manner seems best suited to the material---chronological, or by topic, or
% according to some other criteria (e.g., primary versus secondary resources).
% The examples given in the sections in this chapter are nonsensical in content;
% they are provided merely to give examples of citing bibliographic references.
% Resources should be cited by author name(s), not by title.
% There should be a space between the square brackets of a citation and
% any preceding words. Thus, ``Smith and Jones[17]'' is wrong; ``Smith and
% Jones [17]'' is correct. If the citation is at the end of a sentence, the
% period goes after the brackets (``Johnson [23].'', not ``Johnson. [23]'').

As pseudo-tested methods are a very recent discovery, there has not been much research on this topic. For this reason, this chapter is structured to highlight two studies that support the idea of a pseudo-tested method, as well as the techonologies that support those studies and that have led to the creation of Function-Fiasco.

\section{Primary Sources}

% * Will My Tests Tell Me If I Break This Code?
%   * Stress that this is the first research that acknowledges the idea of pseudo-tested methods
The concept of pseudo-tested methods is a relatively new idea. So recent that the first mention of it is in the paper by Niedermayr and colleagues~\cite{niedermayr2016will} in 2016. Using a technique referred to as extreme mutations, developers were able to discover pseudo-tested methods.
%   * Extreme Mutations
Extreme mutations take the entire functionality out of a function to see how the system or test suite reacts. In this case they would remove the functionality from functions to see if the test cases that were present in the system would still pass, indicating that the function was being pseudo-tested.
%   * Removal process and reasoning
The removal process used kept mutations very low for each method, as this would limit the number of mutants present. At most two mutations per method were desired. If a method had a void return, the entire body was removed from it so it did not have any side effects to the rest of the program. For functions with primitive or string returns, two mutations were created with only return statements with an arbitrary value of the type to be returned. For complex objects, either a generated object was returned or the method was ignored.
%   * Running of the mutated code
After the mutations were created, the mutated code would then be run. Very specifically though, the test cases that the mutation had occured in would be run so as to not make the testing run noisy. This was to help limit the execution time. Researchers were concerned that one of the mutations they created would be equivalent to another mutant as this would radically change the methods which could easily lead to equivalent mutants that were hard to detect because there were many of them\cite{niedermayr2016will}.

%   * Evaluation Strategy
Researchers then began their evaluation strategy. They chose to observe 14 different systems that met the criteria of being a Java based system and were being tested by either the JUnit or testNG frameworks, to test the idea that complete removal of functionality may still produce passing test cases. JUnit and testNG are testing frameworks for the Java language. They observed the mutation test runs to determine answers to the following questions: ``What is the ratio of pseudo-tested methods?'', ``Does the ratio of pseudo-tested methods depend on the type of test?'', and ``How severe are the pseudo-tested methods?''

To answer the question of ``What is the ratio of pseudo-tested methods?'', they comprised a formula based on the mutations.

\begin{quote}
\begin{equation}
 \mbox{\emph{r(Pseudo-tested Methods)}}= \frac{\mbox{\emph{Number of Pseudo-tested Methods}}}{\mbox{\emph{\# of mutated test-executed methods}}}
\end{equation}
\end{quote}

This formula provided a number as to how many pseudo tested methods existed in the tested systems. The result was that 9\%-19\% of the tested methods were pseudo-tested in systems that are tested using unit tests. It was concluded that this does not indicate that coverage is not a definitively a misleading metric for test effectiveness at the method level, as there could be scenarios that could lead to coverage misleading the developer. The next question that the researchers looked to answer was ``Does the ratio depend on the type of test?'' They found that the mean ratio of unit tests was 11.41\% and that the mean ratio of system tests was 35.48\%. This result indicates that the type of test used does change the amount of pseudo-tested methods present. The final question that they aimed to answer was ``How severe are the pseudo-tested methods?'' To answer this question they observed the name of the function being tested and what its operation was. They then indicated a severity of the funtion to the functional category. They found that for 11 of the study objects more than half of the pseudo-tested methods were indicated as either medium or high-severity. This indicates that many of the most important functions in the tested systems were not being tested effectively.

Overall the research indicated that code coverage is not a completely misleading indicator of test effectiveness, and that the number of pseudo-tested methods is a very relevant issue that needs to be researched to a greater extent~\cite{niedermayr2016will}. This conclusion is very important in the field of testing. It indicates that the way developers were testing was not effective enough, and there are pseudo-tested methods in a large amount of open-source projects. Additionally results suggest that it may not be the same ratios and the number of pseudo-tested methods in open-source projects does not indicate the number of pseudo-tested methods in closed-source projects. However there is a high probability that there are pseudo-tested methods in complex closed-source systems.

This literature drastically influenced the way that Function-Fiasco was implemented. In particular, the way that they that they handled the mutation creation. Deciding the return type of the functions is really important for Function-Fiasco, especially because of the loosely typed nature of Python. Function-Fiasco needs to determine what the type of the return will be so that it can create the mutations that may result in a pseudo-tested method. Another really important influence was the idea of an extreme mutation. Function-Fiasco is not relying only on extreme mutations as Python could experience a passing test case even if the return type is changed. The final idea that Function-Fiasco derived from~\cite{niedermayr2016will} is the idea that coverage is not a completely misleading metric. Coverage is very useful and necessary when it comes to the amount of a system that is being observed in a test suite. Function-Fiasco still provides the initial coverage of the system, but it will also adjust it so that developers understand how much of the system is being tested with a high fault detection effectiveness.

Another very important piece of research that was influential in the conception and creation of Function-Fiasco is a paper titled: \textit{A Comprehensive Study of Pseudo-tested Methods}~\cite{vera2017comprehensive}. This is an analysis of pseudo-tested methods, and was used to support the initial research by Niedermayr and colleagues~\cite{niedermayr2016will}. To investigate whether or not pseudo-tested methods are indicators of badly tested code and if pseudo-tested methods are indicators of places that a test suite can be improved, researchers used 21 open source projects that contained approximately 28,000 methods under study. It was very important for the researchers to help eliminate the internal risk that Niedermayr and colleagues~\cite{niedermayr2016will} had. They used an external tool to detect the pseudo-tested methods, it was named Descartes.

Four questions are being answered by this research: ``how frequent are pseudo-tested methods'', ``are pseudo-tested methods the weakest points in the program'', with respect to the test suite, ``are pseudo-tested methods helpful for developers to improve the quality of the test suite'', and ``which pseudo-tested methods do developers consider worth an additional testing action.'' To begin their research they compiled a list of 21 open source projects that had to meet a certain criteria. Each project had to be active, written in Java with Maven as a main system build, tested with the JUnit test framework, and available on a version control hosting service with the most common being Github. Once their list was selected, they developed a list of metrics that would allow them to perform the quantitative results. These metrics helped form the metrics used in Function-Fiasco. They consisted of information that would be readily availiable in each system such as number of methods (\textit{\#METH}), and number of methods under analysis (\textit{\#MUA}). These metrics were used to aid in the creation of others that would provide insight into how the number of pseudo-tested methods affect the system and its test suite. The first computed metric was the ratio of methods covered by the test suite, or coverage. This calculation was completed by taking the number of covered methods and dividing it by the number of methods in the system. This is the calculation for function coverage.

 The next metric is one of the most important in Function-Fiasco. It is the ratio of pseudo-tested methods in the tested methods (\textit{PS\_RATE}). This ratio indicates the presence of pseudo-tested methods in system. The calculation is performed by taking the number of pseudo-tested methods (\textit{\#PSEUDO}) and dividing it by the number of methods under analysis.

\begin{quote}
\begin{equation}
 \mbox{\emph{PS\_RATE}} = \frac{\mbox{\emph{\#PSEUDO}}}{\mbox{\emph{\#MUA}}}
\end{equation}
\end{quote}

This calculation is very similiar to the one used in Niedermayr and colleagues' research~\cite{niedermayr2016will}. This was done for two reasons: they were supporting the argument made by Niedermayr and colleagues~\cite{niedermayr2016will} and that it is a very good representation of the state and stability of the test suite that developers are using. Function-Fiasco does not compute mutation scores, but the researchers did. They calculated their mutations on the basis of ``if one test failed'' while the mutation was being investigated. Function-Fiasco does not compute a mutation score because it is used for detection of the tests that are not failing. While a test failing is a good thing in mutations, it is not pertinant to the detection of a pseudo-tested method~\cite{vera2017comprehensive}.

The results of testing fully answered the first question, ``how frequent are pseudo-tested values?'' Pseudo-tested methods were discovered in every system that was investigated, including ones that were of high coverage and commits. The results fully support the study completed by Niedermayr and colleagues~\cite{niedermayr2016will}. It was also found that in 14 of the projects investigated, the ratio of the pseudo-tested methods was under 7\% indicating that the number of pseudo-tested methods can be decreased to an amount that is either negligible or non-existant. This was very important to the creation of Function-Fiasco. Both~\cite{niedermayr2016will} and~\cite{vera2017comprehensive} indicated that the number of pseudo-tested methods was relevant in all systems that are based in Java. This basis helped form the hypothesis of: \emph{Since Python is a loosely typed language, there could potentially be a higher concentration of pseudo-tested methods written into systems that are written in Python.} Mutations for a Java system have to be of a specific type for a return. Python does not have this restriction, so mutations of different types may still lead to a passing test indicating the higher prevalence of pseudo-tested methods. It is important to note that the highest concentration of pseudo-tested methods was found in Java based systems that had a low coverage. It was believed that the correlation exists because in general, systems that are better tested have a higher coverage. This observation did not have an influence on Function-Fiasco as it aims to detect pseudo-tested methods, provide a metric that is representative of the fault detection effectiveness, and study whether or not Python systems contain higher ratios of pseudo-tested methods.

The results for Question 2 indicate whether or not the weakest points of a program are pseudo-tested methods. To answer this question mutation testing was used to determine the chances of a mutation planted in pseudo-tested method being detected. The mutation analysis was run and determined how many mutants for a pseudo-tested method were detected. The result was that the mutation score for pseudo-tested methods was significantly lower than the mutation score for non-pseudo-tested methods. This indicates that a mutant has a much higher probability to go undetected than a non-pseudo-tested method. This finding supports the idea that pseudo-tested methods are dangerous and often go unnoticed. If a mutation score is low, that means that the test suite will not detect many of the mutants for a particular pseudo-tested method. Another important point noted in their testing was that a high mutation score for a pseudo-tested method could be a product of trivial exception-raising mutants. This is another term for a runtime error. An example of a runtime error is the wrong type being returned. That is why it is so important for this analysis to be completed on a Python based system. Python does not have the same exceptions that Java systems do and may result in more pseudo-tested methods because of it. Another reason that Function-Fiasco should be used is that researchers noticed that extreme transformations are less susceptible to being trivially detected. This is because they will not raise exception errors. Function-Fiasco is not performing extreme transformations by its definition. An extreme transformation is a transformation that removes the entire body of a function and returning an arbitrary value that is usually of the same type of the normal function return. Function-Fiasco does a transformation that is also based around other data types, as Python is a loosely typed language.

Question 3 is meant to answer the question, ``Are pseudo-tested methods relevant for developers to improve the quality of the test suite?'' This means that the methods that were being pseudo-tested are negatively affecting the quality of the test suite, for example a method that is essential to the system being pseudo-tested under many mutations. This was accomplished by contacting teams from the projects that were being studied in questions 1 and 2. In every case, developers were able to recognize the issues posed by methods being improperly tested. A very interesting point was that if a solution was provided to the development teams, it was happily accepted and a pull request to the teams repository was accepted. However, if a team was not provided a solution, the teams would not expense operating power to fix the pseudo-tested methods. They accepted that enhancing the quality of a test suite is important, but did not believe that it was a priority at the time. This ideology provided an interesting challenge for Function-Fiasco and suggested that Function-Fiasco should be used at the same time as development. Function-Fiasco would not be able to provide a solution to the pseudo-tested methods, it is only able to indicate where the pseudo-testing is happening. Running Function-Fiasco at the same time as development would make the fixing of the test suite a priority as it is a current project for that team. Also, it is still able to be used outside of the development, if the team desires to investigate the quality of their test suite~\cite{vera2017comprehensive}.

To answer Question 4, ``Which pseudo-tested methods do developers consider worth an additional testing action?'', researchers queried the development teams in an effort to determine what was ``important'' enough to update their current test suite. In cases where the team had decided that the pseudo-tested methods were not worth the time it would take to complete additional testing or update the previous tests, it was generally because the method was not crucial enough to the system. It could be because the method was deprecated, automatically generated, or not relavent enough to the system such as an interface method, debugging, or it was not used enough. In cases where the team had decided that the pseudo-tested methods were worth the time, the methods were a necessity to the system. The reasons were usually that it was part of the core functionality of the system, the method was widely used to run other parts of the project, it is used outside of the project, or because the method functionality was only partially implemented indicating that it was still in the development phases of the cycle. This part of the cycle is generally when most of the testing is currently being completed, so it was considered a priority. Question 4 was very interesting as it led to the return of the methods that were being pseudo-tested. Fumction-Fiasco will allow the development teams to decide if they would like to continue with the testing of that portion of the system, or if they would like to let it remain pseudo-tested if it is not crucial~\cite{vera2017comprehensive}.
%
% * A comprihensive study of pseudo-tested methods
%
% * decartes
%
% * fair fuzz

% * pytest
%
% * Software Fault Injection􏰀 Growing 􏰁Safer􏰁 Systems
% * Software Assessment: Reliability, Safety, Testability (New Dimensions In Engineering Series)
% * Software Fault Injection

\section{Recent Results}
% A number of papers \cite{blum67,damon:95,zobel:97} deal with issues
% that are peripheral to the orthogonal case, but Dio\c{s}an and Oltean
% were the first to tackle it directly.
% In their 2009 paper \cite{diosan09},
% Dio\c{s}an and Oltean apply evolutionary techniques to
% the orthogonal widget case, obtaining empirical results that suggest
% an efficient algorithm might be at hand. Their
% approach is characterized by the use of a genetic algorithm to evolve other,
% more problem-specific evolutionary algorithms.

% * decartes

In terms of recent attempts at the automatic detection of pseudo-tested methods, there has only been one real attempt. It is a very new idea conceptually and has not had much investigation. Therefore this section will describe the one attempt that has been made and some of the technologies that Function-Fiasco is based around.

There has been one attempt at a technology that is able to detect pseudo-tested methods automatically. Descartes is a tool to automatically detect pseudo-tested methods in Java programs tested with JUnit test suites~\cite{vera2018descartes}. It accomplishes this task by using mutation analysis, which is the process of inserting bugs into systems in the form of small changes, they are then run to see their effect on the system at large. The variation of mutation testing that Descartes uses is called an extreme mutation. Extreme mutations were first introduced in the study performed by Niedermayr and colleagues~\cite{niedermayr2016will}. This process is a much more ``coarse-grained'' approach as it eliminates the overall functionality of methods by removing the entire body from it. Extreme mutations were chosen by the development team for a few reasons: it produces less mutants, creates mutants that have a lower possibility of being equivalent, and it operates at the method level to help decide which of the methods is being tested the worst. The determination of a pseudo-tested method was followed as determined in the Niedermayr research~\cite{niedermayr2016will} which was that if all mutants of a function were not detected, the method is considered to be pseudo-tested. The development team operated their analysis as a mutation engine for PITest, which is a practical mutation testing tool for Java~\cite{coles2016pit}. By definition a mutation engine is a plugin that handles the discovery and creation of mutants. Descartes will remove the entire body of a function and either return a primitive value or operate under its special cases. Descartes is configured to return primitive values when a function is meant to return a primitive value, such as returning a \texttt{string} or an \texttt{int}. There are two special cases, one is when a \texttt{void} return is met and the other is when an \texttt{array} needs to be returned. If a void function return is detected, the body of the function is removed and no further action will be taken. If an array return is encountered the body of the function is removed and an empty generic array will be returned.

The development team made their own mutation engine instead of using the default mutation engine that PITest has, Gregor. Gregor is a standard mutation enginge that performs traditional mutation analysis. The mutation operators are created at instruction level, instead of method level, which is what is needed to perform detection on a pseudo-tested method. A very interesting observation was that Descartes made significantly less mutants, ran for less time, and still had a higher mutation score than Gregor. Both of the tools were compared on the same testing programs~\cite{niedermayr2016will}.
% Pseudo-tested method difference
The tool was not made to only produce a mutation score of the different functions. As defined previously, a pseudo-tested method can be found if all mutants created will not be detected by the current test. This is the prime example as to why the team used extreme mutation over Gregor. In Java, only two mutation values are required to verify the existance of a pseudo-tested method. During the examination between the two, Gregor made 45 mutants to the extreme mutation's two. Another very interesting observation is that in some cases both mutants in the extreme mutation were detected, but Gregor made more that went undetected during the analysis. This indicates that extreme mutation is not testing the edge cases of systems~\cite{niedermayr2016will}.

This is why Function-Fiasco is not doing extreme mutations alone. Since Python is a loosely typed language, there needs to be more testing than just the removal of the body of a function and an aribtrary return of the same type. Function-Fiasco performs mutations that affect the type of return, so that the tests are being tested as opposed to the code. This is to prevent an exception from occuring that the system may experience, but is caught. It will also use extreme mutations to help determine if a method is pseudo tested. It could be considered its base case, as if a method is pseudo-tested and detectable, the system can stop running analysis on that case. Edge cases are caught in this way, and the determination of what is returned is by the technique of fuzzing~\cite{niedermayr2016will}.

% * fair fuzz
Fuzzing is the act of providing random inputs to the system to help show where the system has not been tested enough. Fuzzing can expose sections of systems to faults that were unexpected, but will cause unexpected behavior and even fatal crashes. To fuzz test, a developer may randomize input into a certain function to determine if it is stable. One system that has accomplished this type of technique is called Fair Fuzz~\cite{lemieux2018fairfuzz}. FairFuzz~\cite{lemieux2018fairfuzz} is a targeted mutation strategy that is meant to enhance the greybox coverage of a system through the use of fuzzing. Greybox coverage is the coverage of a system that is partially transparent~\cite{karamcheti2018adaptive}. This means that FairFuzz is a tool that uses fuzzing to help determine where the ``rare branches'' are in a system. Rare branches are paths of the system that are not traversed very often and contain under-explored functionalities of the system to show developers where to test. Once the rare branches have been identified, the mutation strategy changes. Fair Fuzz will change the mutation operation so that the rare branch condition remains satisfied. This is to keep the path open and allow fuzzing to happen below this level, in the functions that are guarded by certain conditions. This allows them to find areas that have not been tested as well as the other portions of the system. This approach is meant to find ways to increase the coverage of a system, as coverage is an indication of how much of the system is being executed. Coverage has also been found by Niedermayr and colleagues to have not been a completely misleading metric and should still be considered~\cite{lemieux2018fairfuzz}.

Function-Fiasco uses fuzzing, but not in the way that Fair Fuzz does. It is used to help find areas that have not been thoroughly tested for, but it does not do it in a manner that is expected. Fair Fuzz uses fuzzing on the system as a whole to help find areas that have been ignored by the current test cases. Function-Fiasco uses fuzzing at the method level to help find edge cases that could produce errors that the current test cases do not test for. An edge case is a condition that is met very rarely. An edge case could be a pseudo-tested method as it very rarely happens so it may not be tested for. Function-Fiasco creates random inputs to induce errors to see if that type of mutation will not be detected, thus indicating a pseudo-tested method. This methodology will ensure that the maximum amount of pseudo-tested methods is being detected and that mutations that are of a certain type are not being ignored during the analysis of a system~\cite{lemieux2018fairfuzz}.

The core idea of Function-Fiasco is based around the idea of purposely providing faulty input to a system to see how it will react. This is exactly what Function-Fiasco is doing to the test cases. Function-Fiasco is providing input that is meant to cause failing test cases to see how they will react. Fault-injection is the most widely accepted technique for assessing software robustness and error handling mechanisms of embedded systems~\cite{khosrowjerdi2018virtualized}. This testing technique, as well as others, is usually completed in the later stages of the development cycle because the test suite is usually created and most if not all of the functionality has been instituted. Fault injection is usually performed with a limited scope and coverage, so that current injections are not affecting other parts of the system and producing skewed results when looking for errors. This type of scoping is referred to as the method level. An example of automated fault injection can be found in the research by Feinbube and colleagues~\cite{feinbube2018software}.

Researchers studied how fault injection works and what it takes for a system to be fully fault tolerant. Researchers describe the different methodologies fault injection can use to ensure that the system is fault tolerant. One methodology is introduced as time-based. Time based fault injection is providing faulty inputs at a predetermined time interval. An instance where this may be an issue is if a lengthy computation is taking place and the system tries to write to a database at the same time. This could lead to a failure of one or both of the actions. Another fault injection methodology is location based. Location based fault injection is inserting faulty values into to predefined memory locations. An example of location based fault injection would be if two parts of the system are trying to write to a memory space at the same time. The final approach to fault injection is execution-driven, which is the type of fault injection that Function-Fiasco is using. Execution-driven fault injection is the idea that fault injection occurs dynamically, and it depends on the control flow of the system. Researchers concluded that software dependability is harder to ensure in complex systems with many layers of abstraction, interacting components, concurrency, and increased distribution~\cite{feinbube2018software}. This complexity and inability to ensure software dependability may be solved by the use of fault injection. This solution was introduced by the process created known as \textit{Fault Injection Driven Development (FIDD)}, which is a structured approach to the idea of fault-injection. To implement this approach two things need to be known: a dependability model, which is the intended behavior in the presence of faults, and the failure cause model, which is what causes a system to fail such as errors and fault activation conditions.

In the case of Function-Fiasco, both of these models are known. For the dependability model, it is expected that when a fault is introduced into the system, the result of the execution will be a failing test case. Since the fault injection is being run at a method level, there is no other indicator to look to for the end result. The failure cause model is what will make the system fail, or what is expected to make it fail. Function-Fiasco treats the mutations that are being created as the failure causes. This is because there is unexpected input being supplied to the test cases that is supposed to make them fail. What makes Function-Fiasco so unique is that when there is a passing test case, fault-injection has failed and succeeded at the same time. When it does not produce a failing test case, that is an indicator that the method that is currently being observed is being pseudo-tested. Function-Fiasco uses a fault injection mentality without using a fault injection result~\cite{feinbube2018software}.

% * pytest
Another technology that is used in Function-Fiasco is a Python testing framework named Pytest. Pytest is a robust testing tool that can be used for all types and levels of software testing. It is becomming one of the most widely used testing frameworks because of the powerful features that it has to offer. These include ``assert'' rewriting, a third-party plugin model, and a powerful fixture model~\cite{okken_2018}. It is used as a command line tool that will automatically find tests that developers have written, evaluate them, and provide results of how many had failed and the reasons why. Pytest was a very clear choice when it came to selecting a testing framework for Function-Fiasco. One major driving force is the growing popularity. As Pytest continues to become more popular that means that there will be more systems that use the framework to test their functionality. This means that Function-Fiasco would have the capability to make the testing process of these projects easier and more accurate. The other reason that Pytest was chosen was ease of usage in Python. Since it is a command-line tool and Python is an interpreted language, it can be run during the execution of Function-Fiasco~\cite{okken_2018}.

Function-Fiasco uses Pytest during its runtime to help determine which of the methods being tested are pseudo-tested. This is because of the reporting that Pytest offers. Also the way that the tests are written provide Pytest a very easy way to discern what functions are being tested. This is part of the assert system. A test can be written simply by using an assert that indicates what the correct value should be. Data from the actual system is gathered through function calls that are able to receive information that is calculated by the actual functionality of the system. Function-Fiasco will provide the incorrect value to these function calls, thus mutating the inputs that the tests receive. Function-Fiasco is able to check the Pytest report to determine if a test passed when it shouldn't have, then compare it to the number of methods that are being evaluated. So if a test is being used to ensure that a calculation is being done correctly, and the method that is performing that calculation is modified, Function-Fiasco will be able to discern whether or not the function is pseudo-tested~\cite{okken_2018}.

Software Mutation is a fault-based methodology that uses mutations to program statements to determine the accuracy of test cases. The process of mutation testing involves creating multiple iterations of program statements, which is called a mutant. A mutant in the idea of fault injection is meant to produce a fault in the system. A mutant is created by using a conditional based system that forces a change so that it adheres to a certain rule. Mutants are meant to change code in a way that is recogizable as to why the functionality changed in the manner that it did. That is why in common practice a mutant will only change in a way that is small and quanitifiable so that there is a measurable degree of how much the code has been modified~\cite{friedman_voas_1995}.

There are three types of mutation testing, and they each pertain to how a mutant has been detected during the analysis. Strong mutation testing compares the output of the original program run and the mutated program run. After this comparison has occurred, if the output has changed the mutant can be considered detected. Weak mutation testing compares the data state after both have been run. In the case of Function-Fiasco, this will be indicated by whether or not a test has passed or failed. To determine if the the mutant was detected, if the data state is not the same as the initial program run, the mutant can be considered detected. The final mutation technique is firm mutation testing. This technique does not compare the output as it does in strong mutation testing, but it compares the data state after such as in weak mutation testing. There are benefits to both weak and strong mutation testing. Weak mutation testing runs less code, is less system expensive, and is easier to verify the exact reasoning as to why the mutant was detected and the specific portion of the system that it occurred in. The benefits of strong mutation testing are that the outputs of systems are very easy to verify. This is because it is easy to do a file comparison. Also, the strong mutation testing is more effective in the grand scheme as mutants will be easier to detect. Function-Fiasco was based around the idea of weak mutation testing, mainly because of the runtime. Function-Fiasco runs several mutants during its analysis, so the system needs to be able to compute a high volume. Also, because of the ease of verification of Pytest as to how many of the test have failed, it is very easy to determine after the running of the test as to how many of the mutants were detected~\cite{friedman_voas_1995}.

The way that a mutation score is calculated is based on the determination of how many of the mutants were detected. The different states of a mutant are considered in the calculation, these states are (detected, undetected, and equal). Equal is the amount of states that were equivalent to the running of the normal program. The states of detected and undetected are describing whether or not the system was successful in catching a mutant that affects the functionality of a system.

\begin{quote}
The computation is calculated by dividing the number of mutants detected by the difference of the total mutants and the equivalent mutants.~\cite{malaiya2002software}.

\begin{equation}
\mbox{\emph{Mutation Score}} = \frac{\mbox{\emph{Detected}}}{\mbox{\emph{Total Mutants - Equal Mutants}}}
\end{equation}
\end{quote}

This calculation is representative of the ability of the test suite to detect actual produceable changes that the system may produce\cite{friedman_voas_1995}. Function-Fiasco will not produce a mutation score, as a pseudo-tested method may not be indicated by a mutation score alone. A pseudo-tested method could be indicated by a mutation score, but since Function-Fiasco is meant to detect pseudo-tested methods, the mutation score is not necessary.

% http://pages.cs.wisc.edu/~bart/includes/publications.html
% https://users.ece.cmu.edu/~koopman/ballista/index.html
% http://lcamtuf.coredump.cx/afl/
